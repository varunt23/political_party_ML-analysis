{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 8836: expected 4 fields, saw 5\\n'\n",
      "b'Skipping line 535882: expected 4 fields, saw 7\\n'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import networkx\n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import WordCloud \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "Sentiment= pd.read_csv(\"SAD.csv\", error_bad_lines = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578612"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    790177\n",
       "0    788435\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentiment.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words = None, max_features = 100000, ngram_range = (1,3))\n",
    "lr = LogisticRegression()\n",
    "\n",
    "\n",
    "def LogisticRegression_kfoldcv(splits, X, Y, pipeline, average_method):\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=777)\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for train, test in kfold.split(X, Y):\n",
    "        lr_fit = pipeline.fit(X[train], Y[train])\n",
    "        prediction = lr_fit.predict(X[test])\n",
    "        scores = lr_fit.score(X[test],Y[test])\n",
    "        \n",
    "        accuracy.append(scores * 100)\n",
    "        precision.append(precision_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('             negative     positive')\n",
    "        print('precision:',precision_score(Y[test], prediction, average=None))\n",
    "        recall.append(recall_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('recall:   ',recall_score(Y[test], prediction, average=None))\n",
    "        f1.append(f1_score(Y[test], prediction, average=average_method)*100)\n",
    "        print('f1 score: ',f1_score(Y[test], prediction, average=None))\n",
    "        print('-'*50)\n",
    "\n",
    "    print(\"accuracy: %.2f%% (+/- %.2f%%)\" % (np.mean(accuracy), np.std(accuracy)))\n",
    "    print(\"precision: %.2f%% (+/- %.2f%%)\" % (np.mean(precision), np.std(precision)))\n",
    "    print(\"recall: %.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n",
    "    print(\"f1 score: %.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "original_pipeline = Pipeline([\n",
    "    ('vectorizer', tvec),\n",
    "    ('classifier', lr)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             negative     positive\n",
      "precision: [0.82698251 0.81513863]\n",
      "recall:    [0.81120194 0.83065884]\n",
      "f1 score:  [0.81901622 0.82282555]\n",
      "--------------------------------------------------\n",
      "             negative     positive\n",
      "precision: [0.82944874 0.81730656]\n",
      "recall:    [0.81335811 0.83312663]\n",
      "f1 score:  [0.82132462 0.82514077]\n",
      "--------------------------------------------------\n",
      "             negative     positive\n",
      "precision: [0.82814961 0.81473749]\n",
      "recall:    [0.81034581 0.83221438]\n",
      "f1 score:  [0.81915098 0.82338321]\n",
      "--------------------------------------------------\n",
      "             negative     positive\n",
      "precision: [0.82746305 0.8140137 ]\n",
      "recall:    [0.80958481 0.83156263]\n",
      "f1 score:  [0.81842631 0.82269459]\n",
      "--------------------------------------------------\n",
      "             negative     positive\n",
      "precision: [0.82906586 0.81674548]\n",
      "recall:    [0.81273028 0.83280286]\n",
      "f1 score:  [0.8208168  0.82469602]\n",
      "--------------------------------------------------\n",
      "accuracy: 82.18% (+/- 0.11%)\n",
      "precision: 82.19% (+/- 0.10%)\n",
      "recall: 82.18% (+/- 0.11%)\n",
      "f1 score: 82.17% (+/- 0.11%)\n"
     ]
    }
   ],
   "source": [
    "LogisticRegression_kfoldcv(5, Sentiment.SentimentText, Sentiment.Sentiment, original_pipeline, 'macro') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment SentimentSource  \\\n",
       "0       1          0    Sentiment140   \n",
       "1       2          0    Sentiment140   \n",
       "2       3          1    Sentiment140   \n",
       "3       4          0    Sentiment140   \n",
       "4       5          0    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3            .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Sentiment.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 8 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed:   42.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: 0.705187 using {'bow__ngram_range': (1, 1), 'classifier__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "\n",
      "\n",
      "Mean: 0.689625 Stdev:(0.014710) with: {'bow__ngram_range': (1, 1), 'classifier__alpha': 0.01, 'tfidf__use_idf': True}\n",
      "Mean: 0.705187 Stdev:(0.014644) with: {'bow__ngram_range': (1, 1), 'classifier__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "Mean: 0.685125 Stdev:(0.015297) with: {'bow__ngram_range': (1, 1), 'classifier__alpha': 0.001, 'tfidf__use_idf': True}\n",
      "Mean: 0.694688 Stdev:(0.015252) with: {'bow__ngram_range': (1, 1), 'classifier__alpha': 0.001, 'tfidf__use_idf': False}\n",
      "Mean: 0.689500 Stdev:(0.009140) with: {'bow__ngram_range': (1, 2), 'classifier__alpha': 0.01, 'tfidf__use_idf': True}\n",
      "Mean: 0.704125 Stdev:(0.007669) with: {'bow__ngram_range': (1, 2), 'classifier__alpha': 0.01, 'tfidf__use_idf': False}\n",
      "Mean: 0.681625 Stdev:(0.008409) with: {'bow__ngram_range': (1, 2), 'classifier__alpha': 0.001, 'tfidf__use_idf': True}\n",
      "Mean: 0.689500 Stdev:(0.006624) with: {'bow__ngram_range': (1, 2), 'classifier__alpha': 0.001, 'tfidf__use_idf': False}\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes Classifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(Sentiment['SentimentText'][:20000], Sentiment['Sentiment'][:20000], test_size=0.2)\n",
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(strip_accents='ascii',\n",
    "                            stop_words='english',\n",
    "                            lowercase=True)), \n",
    "    ('tfidf', TfidfTransformer()),  \n",
    "    ('classifier', MultinomialNB()),\n",
    "])\n",
    "\n",
    "parameters = {'bow__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'classifier__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=10, param_grid=parameters, verbose=1)\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "print(grid.best_score_, grid.best_params_)\n",
    "print('\\n')\n",
    "means = grid.cv_results_['mean_test_score']\n",
    "stds = grid.cv_results_['std_test_score']\n",
    "params = grid.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(mean, stdev, param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['twitter_sentiment3.pkl']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(grid, \"twitter_sentiment3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.7195\n",
      "\n",
      "\n",
      "confusion matrix: \n",
      " [[1703  474]\n",
      " [ 648 1175]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.78      0.75      2177\n",
      "           1       0.71      0.64      0.68      1823\n",
      "\n",
      "    accuracy                           0.72      4000\n",
      "   macro avg       0.72      0.71      0.71      4000\n",
      "weighted avg       0.72      0.72      0.72      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_NB = joblib.load(\"twitter_sentiment3.pkl\" )\n",
    "\n",
    "y_preds = model_NB.predict(X_test)\n",
    "\n",
    "print('accuracy score: ',accuracy_score(y_test, y_preds))\n",
    "print('\\n')\n",
    "print('confusion matrix: \\n',confusion_matrix(y_test,y_preds))\n",
    "print('\\n')\n",
    "print( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tvec.fit_transform(Sentiment['SentimentText'].values.astype('U'))\n",
    "y = Sentiment.Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 2000\n",
    "x_train, x_validation_and_test, y_train, y_validation_and_test = train_test_split(x, y, test_size=.02, random_state=SEED)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_validation_and_test, y_validation_and_test, test_size=.5, random_state=SEED)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    sentiment_fit = pipeline.fit(x_train, y_train)\n",
    "    y_pred = sentiment_fit.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print (\"null accuracy: {0:.2f}%\".format(null_accuracy*100))\n",
    "    print (\"accuracy score: {0:.2f}%\".format(accuracy*100))\n",
    "    if accuracy > null_accuracy:\n",
    "        print (\"model is {0:.2f}% more accurate than null accuracy\".format((accuracy-null_accuracy)*100))\n",
    "    elif accuracy == null_accuracy:\n",
    "        print (\"model has the same accuracy with the null accuracy\")\n",
    "    else:\n",
    "        print (\"model is {0:.2f}% less accurate than null accuracy\".format((null_accuracy-accuracy)*100))\n",
    "    print (\"train and test time: {0:.2f}s\".format(train_test_time))\n",
    "    print (\"-\"*80)\n",
    "    return accuracy, train_test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer()\n",
    "lr = LogisticRegression()\n",
    "n_features = np.arange(10000,100001,10000)\n",
    "\n",
    "def accuracy2(vectorizer=cvec, n_features=n_features, stop_words=None, ngram_range=(1, 1), classifier=lr):\n",
    "    result = []\n",
    "    print (classifier)\n",
    "    print (\"\\n\")\n",
    "    for n in n_features:\n",
    "        vectorizer.set_params(stop_words=stop_words, max_features=n, ngram_range=ngram_range)\n",
    "        pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        print (\"Validation for {} features\".format(n))\n",
    "        nfeature_accuracy,tt_time = accuracy(pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,nfeature_accuracy,tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "names = [\"Logistic Regression\", \"Linear SVC\", \"LinearSVC with L1-based feature selection\",\"Multinomial NB\", \n",
    "         \"Bernoulli NB\", \"Ridge Classifier\", \"AdaBoost\", \"Perceptron\",\"Passive-Aggresive\", \"Nearest Centroid\"]\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))),\n",
    "  ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    RidgeClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    Perceptron(),\n",
    "    PassiveAggressiveClassifier(),\n",
    "    NearestCentroid()\n",
    "    ]\n",
    "zipped_clf = zip(names,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "def classifier(vectorizer=tvec, n_features=10000, stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n,c in classifier:\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', c)\n",
    "        ])\n",
    "        print (\"Validation result for {}\".format(n))\n",
    "        print (c)\n",
    "        clf_accuracy,tt_time = accuracy2(checker_pipeline, x_train, y_train, x_validation, y_validation)\n",
    "        result.append((n,clf_accuracy,tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result for Logistic Regression\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
      "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
      "                   warm_start=False)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 82.30%\n",
      "model is 31.55% more accurate than null accuracy\n",
      "train and test time: 294.09s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Linear SVC\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 81.96%\n",
      "model is 31.20% more accurate than null accuracy\n",
      "train and test time: 314.30s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for LinearSVC with L1-based feature selection\n",
      "Pipeline(memory=None,\n",
      "         steps=[('feature_selection',\n",
      "                 SelectFromModel(estimator=LinearSVC(C=1.0, class_weight=None,\n",
      "                                                     dual=False,\n",
      "                                                     fit_intercept=True,\n",
      "                                                     intercept_scaling=1,\n",
      "                                                     loss='squared_hinge',\n",
      "                                                     max_iter=1000,\n",
      "                                                     multi_class='ovr',\n",
      "                                                     penalty='l1',\n",
      "                                                     random_state=None,\n",
      "                                                     tol=0.0001, verbose=0),\n",
      "                                 max_features=None, norm_order=1, prefit=False,\n",
      "                                 threshold=None)),\n",
      "                ('classification',\n",
      "                 LinearSVC(C=1.0, class_weight=None, dual=True,\n",
      "                           fit_intercept=True, intercept_scaling=1,\n",
      "                           loss='squared_hinge', max_iter=1000,\n",
      "                           multi_class='ovr', penalty='l2', random_state=None,\n",
      "                           tol=0.0001, verbose=0))],\n",
      "         verbose=False)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 81.95%\n",
      "model is 31.20% more accurate than null accuracy\n",
      "train and test time: 621.41s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Multinomial NB\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 80.33%\n",
      "model is 29.58% more accurate than null accuracy\n",
      "train and test time: 404.44s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Bernoulli NB\n",
      "BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 79.34%\n",
      "model is 28.58% more accurate than null accuracy\n",
      "train and test time: 274.36s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Ridge Classifier\n",
      "RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,\n",
      "                max_iter=None, normalize=False, random_state=None,\n",
      "                solver='auto', tol=0.001)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 81.84%\n",
      "model is 31.09% more accurate than null accuracy\n",
      "train and test time: 324.64s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for AdaBoost\n",
      "AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n",
      "                   n_estimators=50, random_state=None)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 68.76%\n",
      "model is 18.01% more accurate than null accuracy\n",
      "train and test time: 1174.46s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Perceptron\n",
      "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
      "           fit_intercept=True, max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
      "           penalty=None, random_state=0, shuffle=True, tol=0.001,\n",
      "           validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 75.69%\n",
      "model is 24.94% more accurate than null accuracy\n",
      "train and test time: 364.16s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Passive-Aggresive\n",
      "PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=None, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 78.82%\n",
      "model is 28.06% more accurate than null accuracy\n",
      "train and test time: 271.23s\n",
      "--------------------------------------------------------------------------------\n",
      "Validation result for Nearest Centroid\n",
      "NearestCentroid(metric='euclidean', shrink_threshold=None)\n",
      "null accuracy: 50.75%\n",
      "accuracy score: 72.14%\n",
      "model is 21.39% more accurate than null accuracy\n",
      "train and test time: 9718.09s\n",
      "--------------------------------------------------------------------------------\n",
      "CPU times: user 57min 57s, sys: 4min 24s, total: 1h 2min 21s\n",
      "Wall time: 3h 49min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trigram_result = classifier(n_features=100000,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set has total 1547039 entries with 49.93% negative, 50.07% positive\n",
      "Validation set has total 15786 entries with 50.75% negative, 49.25% positive\n",
      "Test set has total 15787 entries with 50.22% negative, 49.78% positive\n"
     ]
    }
   ],
   "source": [
    "print (\"Train set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_train),\n",
    "                                                                             (len(x_train[y_train == 0]) / (len(x_train)*1.))*100,\n",
    "                                                                            (len(x_train[y_train == 1]) / (len(x_train)*1.))*100))\n",
    "print (\"Validation set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_validation),\n",
    "                                                                             (len(x_validation[y_validation == 0]) / (len(x_validation)*1.))*100,\n",
    "                                                                            (len(x_validation[y_validation == 1]) / (len(x_validation)*1.))*100))\n",
    "print (\"Test set has total {0} entries with {1:.2f}% negative, {2:.2f}% positive\".format(len(x_test),\n",
    "                                                                             (len(x_test[y_test == 0]) / (len(x_test)*1.))*100,\n",
    "                                                                            (len(x_test[y_test == 1]) / (len(x_test)*1.))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(tweets.index, tweets):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using logistic regression and tfidf vectorizer\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('classifier', lr)\n",
    "])\n",
    "\n",
    "BSBefore = pd.read_csv(\"bs1cleanB.csv\")\n",
    "lr_fit = pipeline2.fit(x1, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetId</th>\n",
       "      <th>CleanText</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1183895217759576065</td>\n",
       "      <td>harvey weinstein reportedly helped clinton cam...</td>\n",
       "      <td>2019-10-14 23:59:50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>manfretti123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1183895200093093888</td>\n",
       "      <td>heard folks petebuttigieg nt want pocket chang...</td>\n",
       "      <td>2019-10-14 23:59:46</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>AlabasterSquare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1183895128508915717</td>\n",
       "      <td>bernie sanders would raise corporate tax rate ...</td>\n",
       "      <td>2019-10-14 23:59:29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TT_Investors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1183894916855939074</td>\n",
       "      <td>harvey weinstein reportedly helped clinton cam...</td>\n",
       "      <td>2019-10-14 23:58:38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>JackieMcReath1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1183894898392666112</td>\n",
       "      <td>bernie sanders calls amazon hometown tax battl...</td>\n",
       "      <td>2019-10-14 23:58:34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>JoziBoySA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TweetId                                          CleanText  \\\n",
       "0  1183895217759576065  harvey weinstein reportedly helped clinton cam...   \n",
       "1  1183895200093093888  heard folks petebuttigieg nt want pocket chang...   \n",
       "2  1183895128508915717  bernie sanders would raise corporate tax rate ...   \n",
       "3  1183894916855939074  harvey weinstein reportedly helped clinton cam...   \n",
       "4  1183894898392666112  bernie sanders calls amazon hometown tax battl...   \n",
       "\n",
       "             timestamp  likes  replies  retweets         username  \n",
       "0  2019-10-14 23:59:50      0        0         0     manfretti123  \n",
       "1  2019-10-14 23:59:46      0        0         0  AlabasterSquare  \n",
       "2  2019-10-14 23:59:29      0        0         0     TT_Investors  \n",
       "3  2019-10-14 23:58:38      0        0         0   JackieMcReath1  \n",
       "4  2019-10-14 23:58:34      0        0         0        JoziBoySA  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSBefore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = tvec.fit_transform(BSBefore['CleanText'].values.astype('U'))\n",
    "\n",
    "\n",
    "prediction = lr_fit.predict(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.615333078101072"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "AKBefore = pd.read_csv(\"ak2cleanB.csv\")\n",
    "AKBefore.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1376"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AKBefore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words = None, max_features = 100000, ngram_range = (1,3))\n",
    "\n",
    "tfidf_transformer = tvec.fit(Sentiment.SentimentText)\n",
    "ak_x = tfidf_transformer.transform(AKBefore['CleanText'].values.astype('U'))\n",
    "\n",
    "ak_x = tvec.fit_transform(AKBefore['CleanText'].values.astype('U'))\n",
    "AKprediction = lr_fit.predict(ak_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AKprediction[1:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     dem candidates congress gates bezos agreed don...\n",
       "2     asked pay thedemocrats answer trump mexico ame...\n",
       "3     troll tweets amy klobuchar adam schiff eric sw...\n",
       "4     tonight demdebate democratic presidential cand...\n",
       "5     want real president vote amy klobuchar quid pr...\n",
       "6     maybe speakerpelosi tell candidates drop since...\n",
       "7     hoping amyklobuchar far better candidate polls...\n",
       "8     hi everyone name amy klobuchar running preside...\n",
       "9     lindseygrahamsc silent photo marie amyklobucha...\n",
       "10    current dems ranking solid warren wavering ewa...\n",
       "11    candidates qualified tonight demdebate former ...\n",
       "12                                   go amyklobuchar go\n",
       "13    watch democratic debate bernie sanders elizabe...\n",
       "14    update amyklobuchar also joining us tomorrow m...\n",
       "15                             still like amy klobuchar\n",
       "16    honestly amyklobuchar oneliners make think may...\n",
       "17    know top get press hope ashleyrparker mitchell...\n",
       "18    kamalaharris joebiden amyklobuchar drop suspen...\n",
       "19    amyklobuchar drop race policies trash nothing ...\n",
       "Name: CleanText, dtype: object"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AKBefore.CleanText[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45421511627906974"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AK_BeforeScore = np.mean(AKprediction)\n",
    "AK_BeforeScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TweetId</th>\n",
       "      <th>CleanText</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>likes</th>\n",
       "      <th>replies</th>\n",
       "      <th>retweets</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1197243090605690881</td>\n",
       "      <td>cnn msnbc go inadvertently forgot mention andr...</td>\n",
       "      <td>2019-11-20 19:59:31</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Restorer_Healer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1197243020535570433</td>\n",
       "      <td>andrew least speaking time every time efficien...</td>\n",
       "      <td>2019-11-20 19:59:14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Shigga11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1197243012604055552</td>\n",
       "      <td>unfortanly already started</td>\n",
       "      <td>2019-11-20 19:59:12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>yanggang101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1197242987903717376</td>\n",
       "      <td>watch democratic debate andrew yang support an...</td>\n",
       "      <td>2019-11-20 19:59:06</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>TheAverageLefty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1197242902721773568</td>\n",
       "      <td>andrew yang promises give americans usd month ...</td>\n",
       "      <td>2019-11-20 19:58:46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>reallyiscnn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TweetId                                          CleanText  \\\n",
       "0  1197243090605690881  cnn msnbc go inadvertently forgot mention andr...   \n",
       "1  1197243020535570433  andrew least speaking time every time efficien...   \n",
       "2  1197243012604055552                         unfortanly already started   \n",
       "3  1197242987903717376  watch democratic debate andrew yang support an...   \n",
       "4  1197242902721773568  andrew yang promises give americans usd month ...   \n",
       "\n",
       "             timestamp  likes  replies  retweets         username  \n",
       "0  2019-11-20 19:59:31      2        0         1  Restorer_Healer  \n",
       "1  2019-11-20 19:59:14      0        0         0         Shigga11  \n",
       "2  2019-11-20 19:59:12      0        1         0      yanggang101  \n",
       "3  2019-11-20 19:59:06      1        0         0  TheAverageLefty  \n",
       "4  2019-11-20 19:58:46      1        0         0      reallyiscnn  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AYBefore = pd.read_csv(\"ay2cleanB.csv\")\n",
    "AYBefore.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words = None, max_features = 100000, ngram_range = (1,3))\n",
    "tfidf_transformer = tvec.fit(Sentiment.SentimentText)\n",
    "\n",
    "ak_x = tvec.fit_transform(AKBefore['CleanText'].values.astype('U'))\n",
    "AKprediction = lr_fit.predict(ak_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45421511627906974"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(AYprediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
